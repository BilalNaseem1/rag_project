{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import together\n",
    "from together import Together\n",
    "# together.api_key = os.environ.get('TOGETHER_API_KEY')\n",
    "load_dotenv()\n",
    "together_api_key = os.environ.get('TOGETHER_API_KEY')\n",
    "\n",
    "client = Together(api_key = together_api_key)\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "# new_system_prompt = \"\"\"\\\n",
    "#     You are an AI assistant tasked with summarizing the key points of a YouTube video transcript into a clear, \\\n",
    "#         concise two-sentence summary. Analyze the transcript to identify the main topic, central message, if any coins are mentioned \\\n",
    "#             in the video capture those also and named entities.\n",
    "#                 \"\"\"\n",
    "\n",
    "new_system_prompt = \"You are a helpful assisyan\"\n",
    "\n",
    "def mistral_7b_model(prompt):\n",
    "    stream = client.chat.completions.create(\n",
    "        # model = \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        # model=\"cognitivecomputations/dolphin-2.5-mixtral-8x7b\",\n",
    "        # model = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\n",
    "        model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        # model = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                # \"content\": \"You are an AI assistant tasked with summarizing the key points of a YouTube video transcript into a clear, \\\n",
    "                #     concise two-sentence summary. Analyze the transcript to identify the main topic, central message, if any coins are mentioned \\\n",
    "                #         in the video capture those also and named entities.\"\n",
    "                \"content\": B_SYS + new_system_prompt + E_SYS\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        stream=True,\n",
    "        temperature=0.01, # This parameter controls the \"creativity\" or \"randomness\" of the model's text generation.\n",
    "        max_tokens=80,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    summary = \"\"\n",
    "    for chunk in stream:\n",
    "        summary += chunk.choices[0].delta.content or \"\"\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"who made you openai or mistal?\"\n",
    "response = mistral_7b_model(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who was leonardo davinci?\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Write a 1-2 sentence summary about {query}\"\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "response = llm_chain.run(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_llm = TogetherLLM(\n",
    "    model= \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_llm), test_llm.model, test_llm.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_llm(\"What are the olympics? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import Together\n",
    "\n",
    "load_dotenv()\n",
    "llm = Together(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    temperature=0.01,\n",
    "    max_tokens=128,\n",
    "    # top_k=1,\n",
    "    top_p=0.9,\n",
    "    together_api_key=  os.environ.get('TOGETHER_API_KEY')\n",
    ")\n",
    "\n",
    "input_ = \"\"\"You are a teacher with a deep knowledge of machine learning and AI. \\\n",
    "You provide succinct and accurate answers. Answer the following question: \n",
    "\n",
    "What is a large language model?\"\"\"\n",
    "print(llm.invoke(input_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = \"\"\"Who are you? who made you?\"\"\"\n",
    "print(llm.invoke(input_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "load_dotenv()\n",
    "client = anthropic.Anthropic(\n",
    "    # # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key = os.environ.get('ANTHROPIC_API_KEY'),\n",
    ")\n",
    "message = client.messages.create(\n",
    "    model= MODEL_NAME,\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n",
    "    ]\n",
    ")\n",
    "print(message.content)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
